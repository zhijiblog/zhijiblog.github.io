<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>对话系统s on 知几小博客</title>
    <link>https://zhijiblog.github.io/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/</link>
    <description>Recent content in 对话系统s on 知几小博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 18 Sep 2020 10:04:31 +0800</lastBuildDate><atom:link href="https://zhijiblog.github.io/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>简介</title>
      <link>https://zhijiblog.github.io/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/%E7%AE%80%E4%BB%8B/%E7%AE%80%E4%BB%8B/</link>
      <pubDate>Fri, 18 Sep 2020 10:04:31 +0800</pubDate>
      
      <guid>https://zhijiblog.github.io/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/%E7%AE%80%E4%BB%8B/%E7%AE%80%E4%BB%8B/</guid>
      <description>项目中使用的主要算法为朴素贝叶斯（Naive Bayes）。贝叶斯方法是一个直接且高效的文本分类方法，主要的思想是条件概率，就是说，在给定一个句子的情况下，我们要计算出它为闲聊类的概率和业务类的概率，并比较二者的大小。概率更大的就为我们的预测结果。
那么问题就转化成了计算一个句子为闲聊或业务的概率。我们以这样一个句子S为例：
· AKM这把枪的后座力大吗？
想要计算这个句子为闲聊或业务的概率，我们需要有两个语料库，分别为闲聊的语料库和业务的语料库。语料库里包含大量我们已经分好类的句子，这就相当于一个知识库，我们可以把它理解成人类的大脑。当我们看到一条句子，我们会在脑内回忆这个句子是作为闲聊出现的次数多，还是业务出现的次数多。也就是把概率转化成频率：
P( 闲聊 | S ) = S在闲聊类语料库里出现的次数 / 闲聊库语料数量；
P( 业务 | S ) = S在业务类语料库里出现的次数 / 业务库语料数量
这两条公式直观上理解，就是在我们已有的经验里面，越经常在某个语料库里出现，就越可能属于这个语料库。
这样计算当然是很简单的，但是存在一个很大的问题：语料库中很难精准匹配上这个句子。虽然语料库非常庞大，但是对于同一个意思，存在很多种表达方式来描述，也就是说**语料库里可能会很多与s意思相近的句子，但是表达方式不一样。**例如“AKM的后座力大不大”， “AKM后座力怎么样”。在统计的时候机器只会去匹配完全相同的句子，这样的话，最后得到的结果就不准确了。
为了解决这个问题，我们就要用到朴素贝叶斯的想法。这个想法简单来说就是：把句子拆开，计算出句子中每个词在语料库中出现的频率，再将它们连乘，当作整个句子的概率。这样就可以规避上面的问题。因为词语和句子不同，虽然存在句子多种表达方式，但是核心的词语只有一种。在上面的例子中，“AKM”，“**后座力”**这几个词都出现了。
这里就要用到（二）中提到的NLP分词技术了。我们把语料库和S都做分词，并过滤掉停用词。此时S只剩下这四个词：
AKM / 枪 / 后座力 / 大
这时我们计算P( 闲聊 | S )，就变成下面这四个频率的乘积：
P( 闲聊 | S ) = (“AKM”在闲聊类语料库里出现的次数 / 闲聊库词语数量) *
(“枪”在闲聊类语料库里出现的次数 / 闲聊库词语数量) *
(“后座力”在闲聊类语料库里出现的次数 / 闲聊库词语数量) *
(“大”在闲聊类语料库里出现的次数 / 闲聊库词语数量)
可以发现，经过这样的处理，成功解决了上面提到的“句子稀疏”问题。其实我们从人的角度也可以理解。当我们人类看到一句话的时候，也是通过句子中的关键词来把握句子的意思的。在这个算法中，我们可以理解为就是把一句话拆开，把里面的关键词放进一个袋子里。这叫做**“词袋子模型”。**
​ 词袋子模型中，词与词之间的顺序不会被纳入考虑。例如“武松打死了老虎”与“老虎打死了武松”被它认作一个意思。这是模型的一个缺点，但是在我们这个项目中并不存在这个问题。因为不会有句子只通过语序的调整就能从闲聊变成业务，说到底还是以词语本身为主，因此这个算法是高效且适用的。</description>
    </item>
    
    <item>
      <title>热门研究方向</title>
      <link>https://zhijiblog.github.io/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/%E7%83%AD%E9%97%A8%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/%E7%83%AD%E9%97%A8%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/</link>
      <pubDate>Fri, 18 Sep 2020 10:04:15 +0800</pubDate>
      
      <guid>https://zhijiblog.github.io/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/%E7%83%AD%E9%97%A8%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/%E7%83%AD%E9%97%A8%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/</guid>
      <description>2.1 环境搭建 目前运用朴素贝叶斯实现的文本分类框架有很多，例如TextBlob、SnowNLP、PaddleHub等。本次项目使用了SnowNLP这个框架。因为这个框架是针对中文文本编写的，且使用起来比较方便。SnowNLP的源码参见https://github.com/isnowfy/snownlp。
具体使用框架的时候，首先需要获取业务类语料库和闲聊类语料库。该项目使用的语料库是从有记录以来，所有的已分类好的提问记录。其中业务类包含437246条真实语料，闲聊类包含149636条真实语料。这两个语料库需要分成两个txt文件保存至项目目录下：
   业务类.txt 闲聊类.txt     升级段位可以获得头像框吗 熊出没之春日对对碰是什么类型的动漫   屏蔽的好友在哪里找回来 007皇家赌场的拍摄日期是那年？   好友之间怎么观战 马绍尔群岛的首都是哪里？   怎么切换回原来的帐号 吴宗宪的出生地是   历史战绩能保存几天 真三国无双7中有几个势力？   两人组队怎么退出房间 祝你做个好梦，睡个好觉。   圆形升级核心从哪儿获得 我说要考试了怎么办   …… ……    2.2 训练集优化 这些原始的语料库只经过了简单的去重，保证了彼此之间不会重复。为了进一步去除语料库中的重复内容，考虑前面提到的去除停用词方法，把句子里的停用词全部去掉，再做一次去重。这样能够进一步降低语料库的冗余性。例如如下两个句子：
 绝版 / 纪念 / 头像框 / 怎么 / 获得 绝版 / 纪念 / 头像框 / 哪里 / 可以 / 获得  两个句子中标红的词语均为停用词，删除之后两个句子都变成了：</description>
    </item>
    
  </channel>
</rss>
