<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>简介 on 知几小博客</title>
    <link>https://zhijiblog.github.io/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/%E7%AE%80%E4%BB%8B/</link>
    <description>Recent content in 简介 on 知几小博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 21 Sep 2020 16:09:16 +0800</lastBuildDate><atom:link href="https://zhijiblog.github.io/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/%E7%AE%80%E4%BB%8B/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>简介</title>
      <link>https://zhijiblog.github.io/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/%E7%AE%80%E4%BB%8B/%E7%AE%80%E4%BB%8B/</link>
      <pubDate>Fri, 18 Sep 2020 10:04:31 +0800</pubDate>
      
      <guid>https://zhijiblog.github.io/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/%E7%AE%80%E4%BB%8B/%E7%AE%80%E4%BB%8B/</guid>
      <description>项目中使用的主要算法为朴素贝叶斯（Naive Bayes）。贝叶斯方法是一个直接且高效的文本分类方法，主要的思想是条件概率，就是说，在给定一个句子的情况下，我们要计算出它为闲聊类的概率和业务类的概率，并比较二者的大小。概率更大的就为我们的预测结果。
那么问题就转化成了计算一个句子为闲聊或业务的概率。我们以这样一个句子S为例：
· AKM这把枪的后座力大吗？
想要计算这个句子为闲聊或业务的概率，我们需要有两个语料库，分别为闲聊的语料库和业务的语料库。语料库里包含大量我们已经分好类的句子，这就相当于一个知识库，我们可以把它理解成人类的大脑。当我们看到一条句子，我们会在脑内回忆这个句子是作为闲聊出现的次数多，还是业务出现的次数多。也就是把概率转化成频率：
P( 闲聊 | S ) = S在闲聊类语料库里出现的次数 / 闲聊库语料数量；
P( 业务 | S ) = S在业务类语料库里出现的次数 / 业务库语料数量
这两条公式直观上理解，就是在我们已有的经验里面，越经常在某个语料库里出现，就越可能属于这个语料库。
这样计算当然是很简单的，但是存在一个很大的问题：语料库中很难精准匹配上这个句子。虽然语料库非常庞大，但是对于同一个意思，存在很多种表达方式来描述，也就是说**语料库里可能会很多与s意思相近的句子，但是表达方式不一样。**例如“AKM的后座力大不大”， “AKM后座力怎么样”。在统计的时候机器只会去匹配完全相同的句子，这样的话，最后得到的结果就不准确了。
为了解决这个问题，我们就要用到朴素贝叶斯的想法。这个想法简单来说就是：把句子拆开，计算出句子中每个词在语料库中出现的频率，再将它们连乘，当作整个句子的概率。这样就可以规避上面的问题。因为词语和句子不同，虽然存在句子多种表达方式，但是核心的词语只有一种。在上面的例子中，“AKM”，“**后座力”**这几个词都出现了。
这里就要用到（二）中提到的NLP分词技术了。我们把语料库和S都做分词，并过滤掉停用词。此时S只剩下这四个词：
AKM / 枪 / 后座力 / 大
这时我们计算P( 闲聊 | S )，就变成下面这四个频率的乘积：
P( 闲聊 | S ) = (“AKM”在闲聊类语料库里出现的次数 / 闲聊库词语数量) *
(“枪”在闲聊类语料库里出现的次数 / 闲聊库词语数量) *
(“后座力”在闲聊类语料库里出现的次数 / 闲聊库词语数量) *
(“大”在闲聊类语料库里出现的次数 / 闲聊库词语数量)
可以发现，经过这样的处理，成功解决了上面提到的“句子稀疏”问题。其实我们从人的角度也可以理解。当我们人类看到一句话的时候，也是通过句子中的关键词来把握句子的意思的。在这个算法中，我们可以理解为就是把一句话拆开，把里面的关键词放进一个袋子里。这叫做**“词袋子模型”。**
​ 词袋子模型中，词与词之间的顺序不会被纳入考虑。例如“武松打死了老虎”与“老虎打死了武松”被它认作一个意思。这是模型的一个缺点，但是在我们这个项目中并不存在这个问题。因为不会有句子只通过语序的调整就能从闲聊变成业务，说到底还是以词语本身为主，因此这个算法是高效且适用的。</description>
    </item>
    
  </channel>
</rss>
