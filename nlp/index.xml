<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLPs on 知几小博客</title>
    <link>https://zhijiblog.github.io/nlp/</link>
    <description>Recent content in NLPs on 知几小博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 18 Sep 2020 15:01:39 +0800</lastBuildDate><atom:link href="https://zhijiblog.github.io/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>简介</title>
      <link>https://zhijiblog.github.io/nlp/%E7%AE%80%E4%BB%8B/</link>
      <pubDate>Fri, 18 Sep 2020 15:01:39 +0800</pubDate>
      
      <guid>https://zhijiblog.github.io/nlp/%E7%AE%80%E4%BB%8B/</guid>
      <description>SnowNLP的技术原理 什么是NLP（Natural Language Processing，自然语言处理）
​ NLP是利用计算机为工具，对人类特有的书面形式和口头形式的自然语言的信息进行各种类型处理和加工的技术。简单来说就是把人类的语言转化为机器语言，再通过计算机进行进一步处理。
目前NLP应用领域很多，在机器翻译（Google翻译）、信息检索（搜索引擎）、信息收取（关键字提取）、文本分类（本次项目内容）、情感分类（好评/差评）、问答系统（Siri、小爱同学）、推荐系统（网易云日推）、阅读理解等领域有着非常广泛的应用。
下面我就这次项目的几个关键技术做详细介绍。
SnowNLP简介 SnowNLP是一个python写的类库，可以方便的处理中文文本内容，是受到了TextBlob的启发而写的。由于现在大部分的自然语言处理库基本都是针对英文的，为了方便处理中文，作者才编写了这个类库。其支持的中文自然语言处理功能如下：
 中文分词 词性标注 情感分析 文本分类 转换成拼音 繁体转简体 提取文本关键词 提取文本摘要  本项目中主要用到了文本分类的功能，具体的算法原理会在下面介绍。
在文本分类之前，还需要对输入的文本进行一些预处理的操作，下面按照顺序来介绍。
文本分词 自然语言处理的第一步就是**“断句”**，我们把它叫做**分词**，也就是把一个完整的句子拆分成一个个不可再分的词语。
​ 中文分词的一个例子
文本分词有很多成熟的算法，有些算法比较复杂，这里就不做介绍了。我就以一个比较容易理解的方法做讲解。这个分词方法叫做基于字符串匹配的分词方法。具体过程如下：
 首先建立一个词典，可以把它理解成《现代汉语词典》，为了让我们后续分词的时候“有据可依” 当需要对一个句子进行分词时，首先将句子拆分成多个部分，将每一个部分在词典中做查找，如果找到，分词成功，否则继续拆分匹配直到成功。 对于多个结果，我们选择词数最少的那个。这样是为了不把句子拆得太散。比如下图这个例子，我们在词典中已经找到了“计算语言学”这个词，那么我们不会再把它拆成“计算”，“语言”，“学”这三个词。  ​ 在上例中，我们在词典中分别找出了句子中的这几个词。在保证总次数最少的情况下，就完成了这个分词的任务。
去除停用词（stop-word） 为节省存储空间和提高搜索效率，在处理自然语言数据之前，我们会自动过滤掉一些对理解文本没有作用的****字或词，例如“了”，“的”，“一个”。我们把这些词叫做**“停用词”**（stop words）。停用词在句子中没有太大的实际意义，因此我们在处理的时候直接把它们**丢弃**。
​ 一些停用词
去除句子中的停用词的方法比较直接。我们会根据已有的知识，建立一张记录停用词的停用词表。这张表存储了汉语中所有的停用词。当我们做完分词之后，把每一个分好的词在这张表里做查找，如果在表里找到了这个词，就直接在原句中把这个词删掉。例如上面那个句子
·	计算语言学/课程/是/两个/课时
在去除停用词的时候，我们在停用词表里找到了“是”这个词。那么我们就把它删掉，最后结果就变成了：
· 计算语言学/课程/两个/课时
这样，留下来的每个词都有实际含义，可以让我们后面的处理变得更加高效。
词性标注 词性标注是指为分词结果中的每个单词标注一个正确的词性的程序，也即确定每个词是名词、动词、形容词或者其他词性的过程。
词性标注是为了让机器更好的理解文本。例如在这个项目中，我们判断业务类提问的一个重要标准就是判断名词。如果提问中包含一些游戏中特有的名词，例如降落伞、P城、枪械，那么就有很大的概率为业务类提问。
需要注意的是在，汉语中，大多数词语只有一个词性，或者出现频次最高的词性远远高于第二位的词性。据说单纯选取最高频词性，就能实现80%准确率的中文词性标注程序。
为了进一步提高准确率，还有如下几种方法：
（1）基于最大熵的词性标注
（2）基于统计最大概率输出词性
（3）基于HMM的词性标注
经过上述的NLP处理之后，我们把汉语的文本初步转换成机器可识别的词块。下一步就正式来介绍本项目涉及到的主算法。</description>
    </item>
    
  </channel>
</rss>
