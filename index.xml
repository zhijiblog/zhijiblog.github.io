<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>知几小博客</title>
    <link>https://zhijiblog.github.io/</link>
    <description>Recent content on 知几小博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 14 Dec 2020 16:20:03 +0800</lastBuildDate><atom:link href="https://zhijiblog.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>对话分析</title>
      <link>https://zhijiblog.github.io/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/%E5%AF%B9%E8%AF%9D%E5%88%86%E6%9E%90/%E5%AF%B9%E8%AF%9D%E5%88%86%E6%9E%90/</link>
      <pubDate>Mon, 14 Dec 2020 16:20:03 +0800</pubDate>
      
      <guid>https://zhijiblog.github.io/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/%E5%AF%B9%E8%AF%9D%E5%88%86%E6%9E%90/%E5%AF%B9%E8%AF%9D%E5%88%86%E6%9E%90/</guid>
      <description>2.1 环境搭建 目前运用朴素贝叶斯实现的文本分类框架有很多，例如TextBlob、SnowNLP、PaddleHub等。本次项目使用了SnowNLP这个框架。因为这个框架是针对中文文本编写的，且使用起来比较方便。SnowNLP的源码参见https://github.com/isnowfy/snownlp。
具体使用框架的时候，首先需要获取业务类语料库和闲聊类语料库。*该项目使用的语料库是从有记录以来，所有的已分类好的提问记录。其中业务类包含*437246条真实语料，闲聊类包含149636条真实语料**。这两个语料库需要分成两个txt文件保存至项目目录下：</description>
    </item>
    
    <item>
      <title>环境配置</title>
      <link>https://zhijiblog.github.io/nlp/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</link>
      <pubDate>Mon, 14 Dec 2020 15:01:56 +0800</pubDate>
      
      <guid>https://zhijiblog.github.io/nlp/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</guid>
      <description>知几小博客网址：https://zhijiblog.github.io/
一、环境部署 本博客使用Hugo框架进行搭建，需要预先安装Hugo框架并添加至环境变量。
安装包地址：https://github.com/gohugoio/hugo/releases
添加环境变量后，命令行输入 hugo version，显示hugo版本表明安装成功。
![image (1)](C:\Users\tencent_v_adjhuang\Downloads\image (1).png)
安装完hugo之后，还需要安装git，因为博客安放在github.io的服务器上。
git安装教程：https://www.cnblogs.com/xueweisuoyong/p/11914045.html
安装完git和hugo之后，将zhijiblog文件夹复制到你的电脑上，就完成了博客后台搭建的全部任务。</description>
    </item>
    
    <item>
      <title>使用指南</title>
      <link>https://zhijiblog.github.io/%E5%BF%83%E5%BE%97%E5%88%86%E4%BA%AB/%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</link>
      <pubDate>Fri, 11 Dec 2020 16:41:28 +0800</pubDate>
      
      <guid>https://zhijiblog.github.io/%E5%BF%83%E5%BE%97%E5%88%86%E4%BA%AB/%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</guid>
      <description>知几小博客网址：https://zhijiblog.github.io/
一、环境部署 本博客使用Hugo框架进行搭建，需要预先安装Hugo框架并添加至环境变量。
安装包地址：https://github.com/gohugoio/hugo/releases
添加环境变量后，命令行输入 hugo version，显示hugo版本表明安装成功。
![image (1)](images\image (1).png)
安装完hugo之后，还需要安装git，因为博客安放在github.io的服务器上。
git安装教程：https://www.cnblogs.com/xueweisuoyong/p/11914045.html
安装完git和hugo之后，将zhijiblog文件夹复制到你的电脑上，就完成了博客后台搭建的全部任务。
二、添加博客 ​	打开本地博客文件夹，双击 create_blog.bat 脚本，添加一篇新的博客。
​	首先选择博客所在的模块：
![image (2)](C:\Users\tencent_v_adjhuang\Downloads\image (2).png)
​	然后输入该模块和博客的名称
![image (3)](C:\Users\tencent_v_adjhuang\Downloads\image (3).png)
​	按下回车，即在本地添加了一篇空的博客。路径为zhijiblog\content\模块名\博客名\博客名.md
![image (4)](C:\Users\tencent_v_adjhuang\Downloads\image (4).png)
​	双击进入编辑，推荐使用Typora编辑markdown文件，也可使用腾讯文档导出markdown格式。
![image (5)](C:\Users\tencent_v_adjhuang\Downloads\image (5).png)
​	保存关闭之后，运行根目录下的post脚本，将本地的博客上传至服务器。
![image (6)](C:\Users\tencent_v_adjhuang\Downloads\image (6).png)
​	脚本运行中会跳出github的登陆界面：
![image (7)](C:\Users\tencent_v_adjhuang\Downloads\image (7).png)
​	用户名：zhijiblog@126.com
​	密码：zhijizhiji123
​	点击login之后，还需要再登陆一次。注意这里的username是zhijiblog，密码保持一致
![image (8)](C:\Users\tencent_v_adjhuang\Downloads\image (8).png)
![image (9)](C:\Users\tencent_v_adjhuang\Downloads\image (9).png)
​	至此，提交完成。登录博客主页即可看到新博文。
![image (10)](C:\Users\tencent_v_adjhuang\Downloads\image (10).png)
三、修改博客 ​	修改一篇已经上传的博客，需要在本地打开md格式的博文进行修改，路径为zhijiblog\content\模块名\博客名\博客名.md
![image (11)](C:\Users\tencent_v_adjhuang\Downloads\image (11).</description>
    </item>
    
    <item>
      <title>初次见面</title>
      <link>https://zhijiblog.github.io/%E5%BF%83%E5%BE%97%E5%88%86%E4%BA%AB/%E5%88%9D%E6%AC%A1%E8%A7%81%E9%9D%A2/%E5%88%9D%E6%AC%A1%E8%A7%81%E9%9D%A2/</link>
      <pubDate>Fri, 11 Dec 2020 15:49:47 +0800</pubDate>
      
      <guid>https://zhijiblog.github.io/%E5%BF%83%E5%BE%97%E5%88%86%E4%BA%AB/%E5%88%9D%E6%AC%A1%E8%A7%81%E9%9D%A2/%E5%88%9D%E6%AC%A1%E8%A7%81%E9%9D%A2/</guid>
      <description>请多关照</description>
    </item>
    
    <item>
      <title>研究方向</title>
      <link>https://zhijiblog.github.io/nlp/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/</link>
      <pubDate>Fri, 11 Dec 2020 11:54:51 +0800</pubDate>
      
      <guid>https://zhijiblog.github.io/nlp/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/</guid>
      <description>1、文本向量化 文本的向量化可谓是NLP进入深度学习时代的标志。所谓文本的向量化（embedding），就是将文本用一定维度的向量来表示，也可以理解为文本的数值化。通过embedding，文本的语义、句法等特征得以表征，便于下游模型的处理。
例如，“人/如果/没有/梦想/，/跟/咸鱼/还有/什么/差别”，向机器学习模型直接输入字符串显然是不明智的，不便于模型进行计算和文本之间的比较。那么，我们需要一种方式来表示一个文本，这种文本表示方式要能够便于进行文本之间的比较，计算等。最容易想到的，就是对文本进行向量化的表示。例如，根据语料库的分词结果，建立一个词典，每个词用一个向量来表示，这样就可以将文本向量化了。
词的向量化，最早尝试是词袋模型，后来证明，词袋模型无法表征词序特征，并且会带来维度灾难；Yoshua Bengio在2003年《A Neural Probabilistic Language Model》一文中提出了一种神经网络的方法，用于语言模型的计算， 词向量作为副产品后来却引起了业界的关注。2008年Collobert和Weston展示了 第一个能有效利用预训练词嵌入的研究工作，他们提出的神经网络架构，构成了当前很多方法的基础。这一项研究工作还率先将词嵌入作为 NLP 任务的高效工具。不过词嵌入真正走向NLP主流还是Mikolov 等人在 2013 年做出的研究《Distributed Representations of Words and Phrases and their Compositionality》。Mikolov 等研究者在这篇论文中提出了连续词袋模型CBOW和 Skip-Gram 模型，通过引入负采样等可行性的措施，这两种方法都能 学习高质量的词向量。基于此，ELMO提出了一种相同词能够根据语境生成不同词向量的模型。高质量的词向量的获得，结合LSTM、CNN等神经网络抽取器，使得NER，文本分类以及信息抽取等任务获得了长足的进步。
此外，基于词向量的思想，从2018年开始，NLP中预训练模型开始流行，BERT、GPT、ALBERT以及XLNET等模型不断刷榜。
2、序列标注任务 序列标注任务是NLP里非常基础和重要的任务，例如分词、NER等都属于序列标注任务，包括一些预测span的阅读理解任务也可归于此列。
分词通常是中文自然语言处理的第一步（随着深度学习模型表征能力越来越强，慢慢证明，分词未必是必要的）；NER是非常重要和基础的信息抽取任务，在非常多的场景中都需要用到，例如聊天机器人中的槽位抽取、文本结构化过程中的实体抽取等等。
早期的序列标注任务，例如分词，NER等主要是用HMM、CRF等机器学习模型；随着深度学习的兴起，LSTM+CRF变成序列标注任务的主流方法；当然，因为LSTM的若干缺点，不少NLP的从业者坚持使用CNN，因而基于膨胀卷积的序列标注模型得以提出。随着transformer的提出，利用BERT等预训练模型做NER这类任务开始变得流行，特别是抽取一些相对较长和复杂的实体，例如地址等。需要特别提出的是，有些情况下，正则匹配也是实体抽取的一种有效手段，可作为补充，例如时间实体等。
3、文本分类 文本分类是一个不难理解的概念，即通过计算机对输入文本进行分类，例如判断“你真是个帅哥啊”这句话是褒义还是贬义。文本分类的应用场景很多，例如情感分类、机器人中的意图识别等。
听上去，分类问题似乎是个不难解决的问题，实际上文本分类有它的难度。当类别非常多或者类别与类别之间差异很小时，文本分类就开始变得困难；再者，有时需要考虑额外特征才能分类正确，例如常常需要根据说话者语气，才能判断“你真是个帅哥啊”这句话是讽刺还是真心的赞美。
早期有一些基于传统机器学习的文本分类，例如基于某种词语特征的的贝叶斯模型，SVM分类器等。
随着深度学习的发展，LSTM+softmax/CNN+softmax模型变成了一种非常流行的文本分类架构，基于此Fasttext、textCNN等便捷高效的开源文本分类工具也开始流行。此外，结合Attention等技巧与概念能够一定程度的提高模型的效果。
文本分类还有另外一种模式，即通过将文本向量化，再通过聚类获得类别，NLTK等开源NLP工具都有便捷的Doc2vec API。如果觉得效果不好，可以试试BERT的【CLS】向量。此外，还可以增加TF-IDF模块，构建更有表达能力的DocVec。
4、信息提取任务 信息提取(IE)的目标是将文本信息转化为结构化信息，起初用于定位自然语言文档中的特定信息。广泛的看，信息提取其实是一个非常宽泛的概念，从文本提出感兴趣的内容就可以称为信息提取。在NLP中常常用实体抽取、关系抽取以及事件抽取等手段进行信息抽取。
实体抽取是序列标记问题，上面已经介绍过，关系抽取以及事件抽取则通常转化为分类的任务。关系抽取常常需要先确认subject以及object。所以，关系抽取任务常常伴随着实体抽取的要求。
早期，信息提取多使用正则和传统的机器学习方法。随着深度学习的快速发展，信息提取技术也开始迅速发展。实体抽取与关系抽取从Pipline的方式进化到end-to-end的方式。使用的特征抽取器也逐步进化，从LSTM/CNN到transformer。需要特别提出的是，BERT在信息抽取方面表现出色，基于BERT和阅读理解任务来做信息抽取，是一种非常别致的方式。</description>
    </item>
    
    <item>
      <title>情感分析</title>
      <link>https://zhijiblog.github.io/nlp/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/</link>
      <pubDate>Fri, 11 Dec 2020 10:46:34 +0800</pubDate>
      
      <guid>https://zhijiblog.github.io/nlp/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/</guid>
      <description></description>
    </item>
    
    <item>
      <title>简介</title>
      <link>https://zhijiblog.github.io/nlp/nlp%E7%AE%80%E4%BB%8B/%E7%AE%80%E4%BB%8B/</link>
      <pubDate>Fri, 18 Sep 2020 15:01:39 +0800</pubDate>
      
      <guid>https://zhijiblog.github.io/nlp/nlp%E7%AE%80%E4%BB%8B/%E7%AE%80%E4%BB%8B/</guid>
      <description>SnowNLP的技术原理 什么是NLP（Natural Language Processing，自然语言处理）
​ NLP是利用计算机为工具，对人类特有的书面形式和口头形式的自然语言的信息进行各种类型处理和加工的技术。简单来说就是把人类的语言转化为机器语言，再通过计算机进行进一步处理。
目前NLP应用领域很多，在机器翻译（Google翻译）、信息检索（搜索引擎）、信息收取（关键字提取）、文本分类（本次项目内容）、情感分类（好评/差评）、问答系统（Siri、小爱同学）、推荐系统（网易云日推）、阅读理解等领域有着非常广泛的应用。
下面我就这次项目的几个关键技术做详细介绍。
SnowNLP简介 SnowNLP是一个python写的类库，可以方便的处理中文文本内容，是受到了TextBlob的启发而写的。由于现在大部分的自然语言处理库基本都是针对英文的，为了方便处理中文，作者才编写了这个类库。其支持的中文自然语言处理功能如下：
 中文分词 词性标注 情感分析 文本分类 转换成拼音 繁体转简体 提取文本关键词 提取文本摘要  本项目中主要用到了文本分类的功能，具体的算法原理会在下面介绍。
在文本分类之前，还需要对输入的文本进行一些预处理的操作，下面按照顺序来介绍。
文本分词 自然语言处理的第一步就是**“断句”**，我们把它叫做**分词**，也就是把一个完整的句子拆分成一个个不可再分的词语。
​ 中文分词的一个例子
文本分词有很多成熟的算法，有些算法比较复杂，这里就不做介绍了。我就以一个比较容易理解的方法做讲解。这个分词方法叫做基于字符串匹配的分词方法。具体过程如下：
 首先建立一个词典，可以把它理解成《现代汉语词典》，为了让我们后续分词的时候“有据可依” 当需要对一个句子进行分词时，首先将句子拆分成多个部分，将每一个部分在词典中做查找，如果找到，分词成功，否则继续拆分匹配直到成功。 对于多个结果，我们选择词数最少的那个。这样是为了不把句子拆得太散。比如下图这个例子，我们在词典中已经找到了“计算语言学”这个词，那么我们不会再把它拆成“计算”，“语言”，“学”这三个词。  ​ 在上例中，我们在词典中分别找出了句子中的这几个词。在保证总次数最少的情况下，就完成了这个分词的任务。
去除停用词（stop-word） 为节省存储空间和提高搜索效率，在处理自然语言数据之前，我们会自动过滤掉一些对理解文本没有作用的****字或词，例如“了”，“的”，“一个”。我们把这些词叫做**“停用词”**（stop words）。停用词在句子中没有太大的实际意义，因此我们在处理的时候直接把它们**丢弃**。
​ 一些停用词
去除句子中的停用词的方法比较直接。我们会根据已有的知识，建立一张记录停用词的停用词表。这张表存储了汉语中所有的停用词。当我们做完分词之后，把每一个分好的词在这张表里做查找，如果在表里找到了这个词，就直接在原句中把这个词删掉。例如上面那个句子
·	计算语言学/课程/是/两个/课时
在去除停用词的时候，我们在停用词表里找到了“是”这个词。那么我们就把它删掉，最后结果就变成了：
· 计算语言学/课程/两个/课时
这样，留下来的每个词都有实际含义，可以让我们后面的处理变得更加高效。
词性标注 词性标注是指为分词结果中的每个单词标注一个正确的词性的程序，也即确定每个词是名词、动词、形容词或者其他词性的过程。
词性标注是为了让机器更好的理解文本。例如在这个项目中，我们判断业务类提问的一个重要标准就是判断名词。如果提问中包含一些游戏中特有的名词，例如降落伞、P城、枪械，那么就有很大的概率为业务类提问。
需要注意的是在，汉语中，大多数词语只有一个词性，或者出现频次最高的词性远远高于第二位的词性。据说单纯选取最高频词性，就能实现80%准确率的中文词性标注程序。
为了进一步提高准确率，还有如下几种方法：
（1）基于最大熵的词性标注
（2）基于统计最大概率输出词性
（3）基于HMM的词性标注
经过上述的NLP处理之后，我们把汉语的文本初步转换成机器可识别的词块。下一步就正式来介绍本项目涉及到的主算法。</description>
    </item>
    
    <item>
      <title>简介</title>
      <link>https://zhijiblog.github.io/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/%E7%AE%80%E4%BB%8B/%E7%AE%80%E4%BB%8B/</link>
      <pubDate>Fri, 18 Sep 2020 10:04:31 +0800</pubDate>
      
      <guid>https://zhijiblog.github.io/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/%E7%AE%80%E4%BB%8B/%E7%AE%80%E4%BB%8B/</guid>
      <description>项目中使用的主要算法为朴素贝叶斯（Naive Bayes）。贝叶斯方法是一个直接且高效的文本分类方法，主要的思想是条件概率，就是说，在给定一个句子的情况下，我们要计算出它为闲聊类的概率和业务类的概率，并比较二者的大小。概率更大的就为我们的预测结果。
那么问题就转化成了计算一个句子为闲聊或业务的概率。我们以这样一个句子S为例：
· AKM这把枪的后座力大吗？
想要计算这个句子为闲聊或业务的概率，我们需要有两个语料库，分别为闲聊的语料库和业务的语料库。语料库里包含大量我们已经分好类的句子，这就相当于一个知识库，我们可以把它理解成人类的大脑。当我们看到一条句子，我们会在脑内回忆这个句子是作为闲聊出现的次数多，还是业务出现的次数多。也就是把概率转化成频率：
P( 闲聊 | S ) = S在闲聊类语料库里出现的次数 / 闲聊库语料数量；
P( 业务 | S ) = S在业务类语料库里出现的次数 / 业务库语料数量
这两条公式直观上理解，就是在我们已有的经验里面，越经常在某个语料库里出现，就越可能属于这个语料库。
这样计算当然是很简单的，但是存在一个很大的问题：语料库中很难精准匹配上这个句子。虽然语料库非常庞大，但是对于同一个意思，存在很多种表达方式来描述，也就是说**语料库里可能会很多与s意思相近的句子，但是表达方式不一样。**例如“AKM的后座力大不大”， “AKM后座力怎么样”。在统计的时候机器只会去匹配完全相同的句子，这样的话，最后得到的结果就不准确了。
为了解决这个问题，我们就要用到朴素贝叶斯的想法。这个想法简单来说就是：把句子拆开，计算出句子中每个词在语料库中出现的频率，再将它们连乘，当作整个句子的概率。这样就可以规避上面的问题。因为词语和句子不同，虽然存在句子多种表达方式，但是核心的词语只有一种。在上面的例子中，“AKM”，“**后座力”**这几个词都出现了。
这里就要用到（二）中提到的NLP分词技术了。我们把语料库和S都做分词，并过滤掉停用词。此时S只剩下这四个词：
AKM / 枪 / 后座力 / 大
这时我们计算P( 闲聊 | S )，就变成下面这四个频率的乘积：
P( 闲聊 | S ) = (“AKM”在闲聊类语料库里出现的次数 / 闲聊库词语数量) *
(“枪”在闲聊类语料库里出现的次数 / 闲聊库词语数量) *
(“后座力”在闲聊类语料库里出现的次数 / 闲聊库词语数量) *
(“大”在闲聊类语料库里出现的次数 / 闲聊库词语数量)
可以发现，经过这样的处理，成功解决了上面提到的“句子稀疏”问题。其实我们从人的角度也可以理解。当我们人类看到一句话的时候，也是通过句子中的关键词来把握句子的意思的。在这个算法中，我们可以理解为就是把一句话拆开，把里面的关键词放进一个袋子里。这叫做**“词袋子模型”。**
​ 词袋子模型中，词与词之间的顺序不会被纳入考虑。例如“武松打死了老虎”与“老虎打死了武松”被它认作一个意思。这是模型的一个缺点，但是在我们这个项目中并不存在这个问题。因为不会有句子只通过语序的调整就能从闲聊变成业务，说到底还是以词语本身为主，因此这个算法是高效且适用的。</description>
    </item>
    
    <item>
      <title>热门研究方向</title>
      <link>https://zhijiblog.github.io/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/%E7%83%AD%E9%97%A8%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/%E7%83%AD%E9%97%A8%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/</link>
      <pubDate>Fri, 18 Sep 2020 10:04:15 +0800</pubDate>
      
      <guid>https://zhijiblog.github.io/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/%E7%83%AD%E9%97%A8%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/%E7%83%AD%E9%97%A8%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/</guid>
      <description>2.1 环境搭建 目前运用朴素贝叶斯实现的文本分类框架有很多，例如TextBlob、SnowNLP、PaddleHub等。本次项目使用了SnowNLP这个框架。因为这个框架是针对中文文本编写的，且使用起来比较方便。SnowNLP的源码参见https://github.com/isnowfy/snownlp。
具体使用框架的时候，首先需要获取业务类语料库和闲聊类语料库。该项目使用的语料库是从有记录以来，所有的已分类好的提问记录。其中业务类包含437246条真实语料，闲聊类包含149636条真实语料。这两个语料库需要分成两个txt文件保存至项目目录下：
   业务类.txt 闲聊类.txt     升级段位可以获得头像框吗 熊出没之春日对对碰是什么类型的动漫   屏蔽的好友在哪里找回来 007皇家赌场的拍摄日期是那年？   好友之间怎么观战 马绍尔群岛的首都是哪里？   怎么切换回原来的帐号 吴宗宪的出生地是   历史战绩能保存几天 真三国无双7中有几个势力？   两人组队怎么退出房间 祝你做个好梦，睡个好觉。   圆形升级核心从哪儿获得 我说要考试了怎么办   …… ……    2.2 训练集优化 这些原始的语料库只经过了简单的去重，保证了彼此之间不会重复。为了进一步去除语料库中的重复内容，考虑前面提到的去除停用词方法，把句子里的停用词全部去掉，再做一次去重。这样能够进一步降低语料库的冗余性。例如如下两个句子：
 绝版 / 纪念 / 头像框 / 怎么 / 获得 绝版 / 纪念 / 头像框 / 哪里 / 可以 / 获得  两个句子中标红的词语均为停用词，删除之后两个句子都变成了：</description>
    </item>
    
    <item>
      <title>主要技术</title>
      <link>https://zhijiblog.github.io/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/%E4%B8%BB%E8%A6%81%E6%8A%80%E6%9C%AF/%E4%B8%BB%E8%A6%81%E6%8A%80%E6%9C%AF/</link>
      <pubDate>Thu, 17 Sep 2020 17:05:09 +0800</pubDate>
      
      <guid>https://zhijiblog.github.io/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA/%E4%B8%BB%E8%A6%81%E6%8A%80%E6%9C%AF/%E4%B8%BB%E8%A6%81%E6%8A%80%E6%9C%AF/</guid>
      <description>定制游戏专属词典
对于本项目而言，我们需要把提问分成闲聊类和业务类。这和一般的文本二分类（正向、负向）有着很大的区别。区分一个提问是否属于和平精英的业务类问题，通常需要判断它是否包含一些游戏中的专有名词，例如枪械的名称、游戏中的地点等。尤其是和平精英这款游戏，用户量非常庞大，场景、道具比较复杂。怎样批量、完善地构建一个专属于本游戏的词典，是一个富有挑战性的问题。
游戏运营节奏频繁，需要及时更新模型
作为一款国民级的手游，游戏本身的运营节奏就很频繁，新版本会不断更新。版本更新客观上会带来很多新词汇，例如新的枪械，新的皮肤。如果不对模型做迭代，就无法识别出这些新的活动和版本。更重要的是，用户本身就会对新版本有很多问题。新版本上线的时间恰好也是用户问题激增的一段时间，如果不能及时对预测模型做更新，将不利于用户体验。
游戏玩家有自己的专属语言，不断冒出新“梗”，需要不断做新词发现
除了游戏版本的迭代会带来新词，用户自身也会带来新词。和平精英的游戏圈本质上是一个亚文化圈，圈子里的玩家会有自己的专属语言，例如一些枪械的昵称：“喷子”，“狗杂”，以及一些别的特色语言“苟进决赛圈”，“莽夫”，“老六行为”等等。这些语言并非游戏内提供的语言，而是玩家经过各种交流，在圈子内被广泛认同并使用的语言，具有特殊性。更重要的是，随着游戏版本更新，越来越多的新“梗”也在不断涌现出来。如何敏锐地嗅探出这些新语言、新词汇，也是本项目的一个难点所在。</description>
    </item>
    
  </channel>
</rss>
